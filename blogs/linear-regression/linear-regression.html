<!DOCTYPE html>
<head> 
<title>About</title>
	
	<meta name="author" content="Victoria Tassie">
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	
	
	<link href="../../css/style.css" rel="stylesheet">
	<link href="../../css/bootstrap.min.css" rel="stylesheet" integrity="sha384-GJzZqFGwb1QTTN6wy59ffF1BuGJpLSa9DkKMp0DgiMDm4iYMj70gZWKYbI706tWS" crossorigin="anonymous">


    <style>
      .bd-placeholder-img {
        font-size: 1.125rem;
        text-anchor: middle;
      }

      @media (min-width: 768px) {
        .bd-placeholder-img-lg {
          font-size: 3.5rem;
        }
      }
    </style>
    <!-- Custom styles for this template -->
    <link href="../../css/cover.css" rel="stylesheet">
	<script type="text/javascript" src="../../js/javascript.js"></script>
	
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
		<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	<style>
		p {
			padding: 0px;
			}
			
		.card {
		background-color: #383838;}
		
	</style>
</head>


<body class="text-center">
    <div class="cover-container d-flex w-100 h-100 p-3 mx-auto flex-column">
  <header class="masthead mb-auto">
    <div class="inner">
      <nav class="nav nav-masthead justify-content-center">
        <a class="nav-link" href="../../index.html">Home</a>
        <a class="nav-link" href="../../about.html">About</a>
        <a class="nav-link active" href="../../blogs.html">Blogs</a>
      </nav>
    </div>
  </header>

  <main role="main" class="inner cover">
    <h1 class="cover-heading">Linear Regression</h1>
    <div class="card-deck">
			<div class="card text-white text-center" onclick="displayPlot1('what')">
			
				<div class="card-header">
					<b>What</b>
				</div>
			
				<div class="card-footer" id="what" style="height:0px; opacity:0;">
					<p>Linear Regression is a type of supervised machine learning used when there is a variable that we want to investigate (aka the <strong>Target Variable</strong>),
					and (a) variable(s) that we think may affect / influence the target variable (aka the <strong> Predictor Variable(s)</strong>.</p>
				</div>
			</div>
			
			<div class="card text-white text-center" onclick="displayPlot1('aim')">
			
				<div class="card-header">
					<b>Aim</b>
				</div>
			
				<div class="card-footer" id="aim" style="height:0px; opacity:0;">
					<p>The aim of linear regression is to investigate the relationship between these aforementioned types of variables, through finding 
					the line of best fit.</p> <p>This is the line that has the least error between a predicted value of the target variable and the actual
					value of the target variable.</p>
				</div>
			</div>
			
			<div class="card text-white text-center" onclick="displayPlot1('error')">
			
				<div class="card-header">
					<b>Error</b>
				</div>
			
				<div class="card-footer" id="error" style="height:0px; opacity:0;">
					<p>Error is the difference between a predicted value and the actual value of the target variable. </p>
					<p>In the case of linear regression, we use the <a href="animation.html" target="_blank" onclick="event.stopPropagation();">Mean Squared Error (MSE)</a>.</p>
					<p>It is important to remember that error is a function of the parameters of our model, that is, the parameters that we choose for
					the model affect the error. As we want to minimise the error, we want to find the turning point / minima of the error function using 
					calculus.</p>
				</div>
			</div>
			
			<div class="card text-white text-center" onclick="displayPlot1('calculus')">
			
				<div class="card-header">
					<b>Calculus</b>
				</div>
			
				<div class="card-footer" id="calculus" style="height:0px; opacity:0;">
					<p>Calculus is a branch of mathematics, focused around how quantities vary with each other, as opposed to the value of the quantity
					itself. It was discovered in the 17th century by Leibniz / Newton.</p>
					<p>We are particularly interested in the gradient of the error function, i.e. how the error varies with the different parameter values.
					 The gradient will provide a direction for the greatest decrease in the error, and we wish to find the point where the error is least,
					 i.e. where the gradient is zero (cannot go any lower).</p>
				</div>
			</div>
			
</div>

<div class="card-deck">

	<div class="card text-white text-center" onclick="displayPlot1('ols')">
			
		<div class="card-header">
			<b>Ordinary Least Square</b>
		</div>
	
		<div class="card-footer" id="ols" style="height:0px; opacity:0;" onclick="displaywork('ols-work','21em',event, '36em','15em')">
			<p>The first method is using OLS - a method which takes advantage of the properties of the MSE error function we are using - the function is
			convex. This means that there is a single solution to minimising the error function, and we can find this by finding the derivative of the
			error function and setting it to zero.</p>
			
			<div id="ols-work" style="height:0px; opacity:0;"> 
				<p>We start with $ MSE = \frac{1}{n} \sum{(y_{pred} - y_i)^2} = \frac{1}{n} \sum{(\beta_0 + \beta_1 x_i - y_i)^2}$
					<br> Then differentiating: $ \frac{\partial MSE}{\partial \beta_0} =  \frac{2}{n} \sum{(\beta_0 + \beta_1 x_i - y_i)}$
					<br> We then put this to zero, as we are finding the minimum point of the function: $0 = \frac{2}{n} \sum{\beta_0 + \beta_1 x_i - y_i}$
					<br> Separating the summations, we get: $ 0 = \sum{\beta_0} + \sum{\beta_1 x_i} - \sum{y_i}$
					<br> Rearranging this, we get: $ 0 = n\beta_0 + n\beta_1 \bar{x} - \sum{y_i}$
					<br>
					<br> The differential with respect to $\beta_1$ is $ \frac{\partial MSE}{\partial \beta_1} =  \frac{2}{n} \sum{(\beta_0 + \beta_1 x_i - y_i)x_i}$
					<br> Using similar techniques, we get: $ 0 = n\beta_0 \bar{x} + \beta_1 \sum{x_i^2} - \sum{x_i y_i}$
					<br> Substituting $ n\beta_0$ in to our second equation, we get: $ 0 = \bar{x}(\sum{y_i} - n\beta_1) + \beta_1 \sum{x_i^2} - \sum{x_i y_i} $ 
					<br>Rearranging, we get: $ \beta_1 (\sum{x_i^2} - n\bar{x}) = \sum{x_i y_i} - n\bar{x}\bar{y} $ 
					<br> Then we divide and rearrange: $ \beta_1 = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{\sum{(x_i - \bar{x})^2}} $
					<br>
				</p>
			</div>
			
			<p> The final result is that we can use the following formula to work out the best estimates of the parameters: $ \beta_1 = \frac{\sum
			{(x_i - \bar{x})(y_i - \bar{y})}}{\sum{(x_i - \bar{x})^2}} $.
			<br> From this, we can use the mean value of  and the mean value of y to calculate $ \beta_0 = \bar{y} - \beta_1 \bar{x} $.</p>
		</div>
	</div>
			
	<div class="card text-white text-center" onclick="displayPlot1('gd')">
			
		<div class="card-header">
			<b>Gradient Descent</b>
		</div>
		
		<div class="card-footer" id="gd" style="height:0px; opacity:0;">
			<p>The second method is suited for when simple linear regression is generalised to the case where there are multiple predictor variables.</p>
			<p> Gradient Descent has a 5 main stages: 
				<ol>
					<li> Initialise the parameters (this can be done randomly, or through an educated guess)</li>
					<li>Calculate the gradient of the error function with respect to each of the parameters</li>
					<li> Adjust the parameters according to the gradient</li>
					<li> Use the new parameters in the model and calculate the error of the model </li>
					<li> Repeat steps 2-4 until there is no significant change in the reduction of the error function</li>
				</ol>
				<br>
				For linear regression, we have already worked out the derivative of the error function with respect to each of the parameters. We use these to provide steps
				for the adjustment of the parameters. 
				<br> We would adjust the parameters in the following way:
				<br> $ \hat{\beta}_0 = \hat{\beta}_0 - \alpha \times \frac{2}{n} \sum{(y_{pred_i} - y_i)} $ 
				<br> $ \hat{\beta}_1 = \hat{\beta}_1 - \alpha \times \frac{2}{n} \sum{(y_{pred_i} - y_i)x_i} $</p>
		</div>
	</div>		

</div>

<div class="card-deck">

	<div class="card text-white text-center" onclick="displayPlot1('ex')">
			
		<div class="card-header">
			<b>Example</b>
		</div>
	
		<div class="card-footer" id="ex" style="height:0px; opacity:0;">
			<p>We will now go through an <a href="linear-regression-example.html" target="_blank">example</a>, with made up data.
		</div>

			
		
	
	</div>
</div>
 </main>

  <footer class="mastfoot mt-auto footer">
    <div class="inner">
	  <p class="link" style="font-size: inherit;"><a href="mailto:vic.tee646@gmail.com">
  Email</a> | <a href="https://www.linkedin.com/in/victoria-tassie-964a10106" target="_blank">LinkedIn</a></p>
  <p>This page was created using <a href="https://getbootstrap.com/">Bootstrap</a>, by <a href="https://twitter.com/mdo">@mdo</a>.</p>
    </div>
  </footer>
</div>
</body>

</html>